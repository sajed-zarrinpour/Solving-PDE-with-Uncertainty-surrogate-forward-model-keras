{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective condunctanse in inhomogeneouse media\n",
    "we want to solve the following equation:\n",
    "\n",
    "$A_{eff}(a) = min_{u(x)} \\int_{[0,1]^d} a(x) ||-\\nabla u(x) + \\xi||_2^2$.\n",
    "\n",
    "the data set were generated previousely using Octave, then seperated to two distinct set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Utilities : \n",
    "import numpy as np\n",
    "import os\n",
    "# Keras :\n",
    "# backend\n",
    "from keras import backend as K\n",
    "# layers :\n",
    "from keras.layers import Input, Dense, add, Activation , Conv1D, Flatten ,GlobalAveragePooling1D , Lambda\n",
    "# model :\n",
    "from keras.models import Model, model_from_json, model_from_yaml\n",
    "# optimizer :\n",
    "from keras import optimizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up network parameteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network input parameters \n",
    "\n",
    "n = 8            # number of the input layer neurons \n",
    "\n",
    "# convolutional layers parameters\n",
    "\n",
    "alpha = 16       # number of channels in convolutional layers\n",
    "ks = 1           # Kernel Size\n",
    "\n",
    "# optimizer (NAdam) parameters\n",
    "\n",
    "lr = 0.005       # learning rate\n",
    "beta_1=0.9       # beta 1\n",
    "beta_2=0.999     # beta 2\n",
    "\n",
    "# model fitting parameters\n",
    "\n",
    "epochs = 100     # epochs\n",
    "batch_size = 150 # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the Data to train and test data sets.\n",
    "\n",
    "* Step 1 : loading the data set\n",
    "* Step 2 : shuffling the data set\n",
    "* Step3  : seperating data and lable parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test :\n",
    "test = np.loadtxt('data/test.csv', delimiter=',')\n",
    "np.random.shuffle(test)\n",
    "d_test = np.array(test[:,0:n])\n",
    "l_test = np.array(test[:,n])\n",
    "\n",
    "#train :\n",
    "train = np.loadtxt('data/train.csv', delimiter=',')\n",
    "np.random.shuffle(train)\n",
    "data = np.array(train[:,0:n])\n",
    "labels = np.array(train[:,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to expand the dimensions of our input data, so it can feed directly to the convolutional layers. Please note that, as I did in NLSE, it could be done in the network with `Reshape` layer too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from (height, width) to (height, width, depth)\n",
    "data = np.expand_dims(data, axis=2) \n",
    "d_test = np.expand_dims(d_test, axis=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to define our model. The model is consists of three part. First part is consists of some convolutional layers with kernel size 1, Second part is a sum-pooling block, and, finally, third part is consists of some convolutional layers with kernel size 1. Note that there is a symmetric relation between first and third parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(?, 8, 1), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputTensor = Input(shape=(n,1))\n",
    "print inputTensor\n",
    "\n",
    "stack = []\n",
    "\n",
    "# Part 1: Convolutional layers:\n",
    "# SubPart 1.1 : Expanding the input to alpha channels and performing convolutions with kernel size of ks\n",
    "stack.append(Conv1D(alpha, ks, strides=1, input_shape=(1,n,1),padding='same', activation='relu')(inputTensor))\n",
    "stack.append(Conv1D(alpha, ks, strides=1, padding='same', activation='relu')(add([stack[len(stack)-1],inputTensor])))\n",
    "stack.append(Conv1D(alpha, ks, strides=1, padding='same', activation='relu')(add([stack[len(stack)-1],inputTensor])))\n",
    "# SubPart 1.2 : Flattening the results into 1 chanel\n",
    "stack.append(Conv1D(1, ks, strides=1, padding='same', activation='linear')(add([stack[len(stack)-1],inputTensor])))\n",
    "\n",
    "# Part 2: Sum-Pooling:\n",
    "# there is no sumpooling direct layer, so we have to use an global average layer first :\n",
    "stack.append(GlobalAveragePooling1D(data_format='channels_last')(add([stack[len(stack)-1],inputTensor])))\n",
    "# and then multiply the result in the total number of elements throughh a lambda layer next:\n",
    "stack.append(Lambda(lambda x: x * data.shape[1])(stack[len(stack)-1]))\n",
    "\n",
    "# Part 3: Convolutions equivalent to Dense layers\n",
    "# SubPart 3.1 : Expanding the input to alpha channels and performing convolution with kernel size 'ks'\n",
    "stack.append(Conv1D(alpha, ks, strides=1, padding='same', activation='relu')(add([stack[len(stack)-1],inputTensor])))\n",
    "stack.append(Conv1D(alpha, ks, strides=1, padding='same', activation='relu')(add([stack[len(stack)-1],inputTensor])))\n",
    "stack.append(Conv1D(alpha, ks, strides=1, padding='same', activation='relu')(add([stack[len(stack)-1],inputTensor])))\n",
    "\n",
    "# SubPart 3.2 : Flattening alpha channels to 1 channel, so the whole part can be viewd as a dense layer\n",
    "#shall the kernel size remain 1? or we can use a more dynamic kernel size here\n",
    "stack.append(Conv1D(1, ks, strides=1, padding='same', activation='linear')(add([stack[len(stack)-1],inputTensor])))\n",
    "\n",
    "# SubPart 3.3 : Flattening the result so it can be fed into the output layer\n",
    "stack.append(Flatten()(stack[len(stack)-1])) \n",
    "\n",
    "# SubPart 3.4 : The output layer which is a n -> 1 dense layer and will give us one single output \n",
    "finalOutput = Dense(1)(stack[len(stack)-1])\n",
    "print finalOutput\n",
    "\n",
    "# Defining the Actual Model\n",
    "model = Model(inputTensor,finalOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the network. The loss function is choosen to be mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "127500/127500 [==============================] - 36s 279us/step - loss: 0.0316 - mean_absolute_error: 0.1150\n",
      "Epoch 2/100\n",
      "127500/127500 [==============================] - 34s 269us/step - loss: 0.0055 - mean_absolute_error: 0.0573\n",
      "Epoch 3/100\n",
      "127500/127500 [==============================] - 34s 271us/step - loss: 0.0048 - mean_absolute_error: 0.0542\n",
      "Epoch 4/100\n",
      "127500/127500 [==============================] - 36s 279us/step - loss: 0.0050 - mean_absolute_error: 0.0546\n",
      "Epoch 5/100\n",
      "127500/127500 [==============================] - 34s 270us/step - loss: 0.0043 - mean_absolute_error: 0.0515\n",
      "Epoch 6/100\n",
      "127500/127500 [==============================] - 34s 267us/step - loss: 0.0041 - mean_absolute_error: 0.0505\n",
      "Epoch 7/100\n",
      "127500/127500 [==============================] - 36s 283us/step - loss: 0.0037 - mean_absolute_error: 0.0475\n",
      "Epoch 8/100\n",
      "127500/127500 [==============================] - 35s 275us/step - loss: 0.0030 - mean_absolute_error: 0.0422\n",
      "Epoch 9/100\n",
      "127500/127500 [==============================] - 35s 277us/step - loss: 0.0014 - mean_absolute_error: 0.0291\n",
      "Epoch 10/100\n",
      "127500/127500 [==============================] - 35s 277us/step - loss: 8.2869e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 11/100\n",
      "127500/127500 [==============================] - 38s 301us/step - loss: 6.2511e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 12/100\n",
      "127500/127500 [==============================] - 39s 306us/step - loss: 5.2398e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 13/100\n",
      "127500/127500 [==============================] - 42s 329us/step - loss: 4.4302e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 14/100\n",
      "127500/127500 [==============================] - 38s 298us/step - loss: 3.2182e-04 - mean_absolute_error: 0.01351s - loss: 3.2960e-04 - mean_absolute_error:  - ETA: 1s - loss: 3.2708e-04 - mean_ab\n",
      "Epoch 15/100\n",
      "127500/127500 [==============================] - 43s 334us/step - loss: 2.9313e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 16/100\n",
      "127500/127500 [==============================] - 43s 335us/step - loss: 2.1409e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 17/100\n",
      "127500/127500 [==============================] - 44s 346us/step - loss: 1.6675e-04 - mean_absolute_error: 0.00940s - loss: 1.6706e-04 - mean_absolute_error: 0.0\n",
      "Epoch 18/100\n",
      "127500/127500 [==============================] - 37s 292us/step - loss: 1.5098e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 19/100\n",
      "127500/127500 [==============================] - 37s 290us/step - loss: 1.1135e-04 - mean_absolute_error: 0.0076\n",
      "Epoch 20/100\n",
      "127500/127500 [==============================] - 37s 293us/step - loss: 9.8924e-05 - mean_absolute_error: 0.0066\n",
      "Epoch 21/100\n",
      "127500/127500 [==============================] - 38s 295us/step - loss: 8.6243e-05 - mean_absolute_error: 0.0063\n",
      "Epoch 22/100\n",
      "127500/127500 [==============================] - 38s 296us/step - loss: 7.7899e-05 - mean_absolute_error: 0.0061\n",
      "Epoch 23/100\n",
      "127500/127500 [==============================] - 37s 289us/step - loss: 7.9353e-05 - mean_absolute_error: 0.0062\n",
      "Epoch 24/100\n",
      "127500/127500 [==============================] - 37s 289us/step - loss: 7.5764e-05 - mean_absolute_error: 0.0063\n",
      "Epoch 25/100\n",
      "127500/127500 [==============================] - 37s 292us/step - loss: 7.3833e-05 - mean_absolute_error: 0.0057\n",
      "Epoch 26/100\n",
      "127500/127500 [==============================] - 37s 291us/step - loss: 6.4221e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 27/100\n",
      "127500/127500 [==============================] - 38s 295us/step - loss: 6.4019e-05 - mean_absolute_error: 0.0057\n",
      "Epoch 28/100\n",
      "127500/127500 [==============================] - 38s 297us/step - loss: 6.0102e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 29/100\n",
      "127500/127500 [==============================] - 37s 292us/step - loss: 5.9618e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 30/100\n",
      "127500/127500 [==============================] - 38s 297us/step - loss: 5.8772e-05 - mean_absolute_error: 0.00533s -\n",
      "Epoch 31/100\n",
      "127500/127500 [==============================] - 36s 284us/step - loss: 5.3085e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 32/100\n",
      "127500/127500 [==============================] - 35s 274us/step - loss: 5.1954e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 33/100\n",
      "127500/127500 [==============================] - 35s 277us/step - loss: 4.9459e-05 - mean_absolute_error: 0.0048\n",
      "Epoch 34/100\n",
      "127500/127500 [==============================] - 37s 288us/step - loss: 5.2325e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 35/100\n",
      "127500/127500 [==============================] - 37s 288us/step - loss: 4.5966e-05 - mean_absolute_error: 0.0047\n",
      "Epoch 36/100\n",
      "127500/127500 [==============================] - 37s 292us/step - loss: 4.4210e-05 - mean_absolute_error: 0.0046\n",
      "Epoch 37/100\n",
      "127500/127500 [==============================] - 38s 300us/step - loss: 4.6117e-05 - mean_absolute_error: 0.0048\n",
      "Epoch 38/100\n",
      "127500/127500 [==============================] - 38s 297us/step - loss: 4.0699e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 39/100\n",
      "127500/127500 [==============================] - 38s 297us/step - loss: 4.3792e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 40/100\n",
      "127500/127500 [==============================] - 38s 300us/step - loss: 3.9698e-05 - mean_absolute_error: 0.0043\n",
      "Epoch 41/100\n",
      "127500/127500 [==============================] - 40s 315us/step - loss: 4.3225e-05 - mean_absolute_error: 0.0047\n",
      "Epoch 42/100\n",
      "127500/127500 [==============================] - 40s 312us/step - loss: 3.9182e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 43/100\n",
      "127500/127500 [==============================] - 40s 316us/step - loss: 4.1616e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 44/100\n",
      "127500/127500 [==============================] - 39s 308us/step - loss: 3.8452e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 45/100\n",
      "127500/127500 [==============================] - 39s 309us/step - loss: 4.0416e-05 - mean_absolute_error: 0.0044\n",
      "Epoch 46/100\n",
      "127500/127500 [==============================] - 43s 334us/step - loss: 4.1311e-05 - mean_absolute_error: 0.0044\n",
      "Epoch 47/100\n",
      "127500/127500 [==============================] - 40s 310us/step - loss: 3.7193e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 48/100\n",
      "127500/127500 [==============================] - 39s 303us/step - loss: 4.4102e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 49/100\n",
      "127500/127500 [==============================] - 35s 278us/step - loss: 3.2692e-05 - mean_absolute_error: 0.0038\n",
      "Epoch 50/100\n",
      "127500/127500 [==============================] - 37s 288us/step - loss: 3.2920e-05 - mean_absolute_error: 0.00401s - loss: 3.2102e-05 - mean_absol\n",
      "Epoch 51/100\n",
      "127500/127500 [==============================] - 39s 306us/step - loss: 3.3513e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 52/100\n",
      "127500/127500 [==============================] - 39s 307us/step - loss: 3.4347e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 53/100\n",
      "127500/127500 [==============================] - 40s 310us/step - loss: 3.5493e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 54/100\n",
      "127500/127500 [==============================] - 40s 317us/step - loss: 3.0937e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 55/100\n",
      "127500/127500 [==============================] - 41s 319us/step - loss: 3.5499e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 56/100\n",
      "127500/127500 [==============================] - 44s 342us/step - loss: 3.2306e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 57/100\n",
      "127500/127500 [==============================] - 44s 345us/step - loss: 3.1744e-05 - mean_absolute_error: 0.0038\n",
      "Epoch 58/100\n",
      "127500/127500 [==============================] - 42s 329us/step - loss: 3.4652e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 59/100\n",
      "127500/127500 [==============================] - 41s 322us/step - loss: 3.0988e-05 - mean_absolute_error: 0.00362s - loss: 3.0729e-05 -\n",
      "Epoch 60/100\n",
      "127500/127500 [==============================] - 39s 305us/step - loss: 3.3176e-05 - mean_absolute_error: 0.0034\n",
      "Epoch 61/100\n",
      "127500/127500 [==============================] - 39s 304us/step - loss: 3.1350e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 62/100\n",
      "127500/127500 [==============================] - 39s 304us/step - loss: 3.0372e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 63/100\n",
      "127500/127500 [==============================] - 38s 301us/step - loss: 3.1000e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 64/100\n",
      "127500/127500 [==============================] - 32s 251us/step - loss: 2.9748e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 65/100\n",
      "127500/127500 [==============================] - 33s 257us/step - loss: 2.9465e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 66/100\n",
      "127500/127500 [==============================] - 34s 270us/step - loss: 3.1988e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 67/100\n",
      "127500/127500 [==============================] - 35s 271us/step - loss: 3.1734e-05 - mean_absolute_error: 0.00371s - loss: 3.0625e-05 - mean_abso\n",
      "Epoch 68/100\n",
      "127500/127500 [==============================] - 35s 272us/step - loss: 2.9825e-05 - mean_absolute_error: 0.0038\n",
      "Epoch 69/100\n",
      "127500/127500 [==============================] - 34s 267us/step - loss: 2.8343e-05 - mean_absolute_error: 0.0038\n",
      "Epoch 70/100\n",
      "127500/127500 [==============================] - 34s 269us/step - loss: 2.9737e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 71/100\n",
      "127500/127500 [==============================] - 34s 268us/step - loss: 3.2556e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 72/100\n",
      "127500/127500 [==============================] - 32s 252us/step - loss: 2.7086e-05 - mean_absolute_error: 0.0034\n",
      "Epoch 73/100\n",
      "127500/127500 [==============================] - 35s 278us/step - loss: 2.9852e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 74/100\n",
      "127500/127500 [==============================] - 36s 282us/step - loss: 2.9585e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 75/100\n",
      "127500/127500 [==============================] - 36s 281us/step - loss: 2.8768e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 76/100\n",
      "127500/127500 [==============================] - 36s 282us/step - loss: 2.6176e-05 - mean_absolute_error: 0.0033\n",
      "Epoch 77/100\n",
      "127500/127500 [==============================] - 36s 283us/step - loss: 3.2806e-05 - mean_absolute_error: 0.00390s - loss: 3.2811e-05 - mean_absolute_error: 0.00\n",
      "Epoch 78/100\n",
      "127500/127500 [==============================] - 37s 290us/step - loss: 2.6885e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 79/100\n",
      "127500/127500 [==============================] - 37s 288us/step - loss: 2.7426e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 80/100\n",
      "127500/127500 [==============================] - 37s 293us/step - loss: 2.8473e-05 - mean_absolute_error: 0.0033\n",
      "Epoch 81/100\n",
      "127500/127500 [==============================] - 37s 287us/step - loss: 2.8125e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 82/100\n",
      "127500/127500 [==============================] - 37s 293us/step - loss: 2.6627e-05 - mean_absolute_error: 0.0031\n",
      "Epoch 83/100\n",
      "127500/127500 [==============================] - 36s 285us/step - loss: 2.8036e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 84/100\n",
      "127500/127500 [==============================] - 37s 292us/step - loss: 2.8691e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 85/100\n",
      "127500/127500 [==============================] - 37s 294us/step - loss: 2.6131e-05 - mean_absolute_error: 0.0031\n",
      "Epoch 86/100\n",
      "127500/127500 [==============================] - 40s 313us/step - loss: 2.6245e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 87/100\n",
      "127500/127500 [==============================] - 38s 299us/step - loss: 2.6968e-05 - mean_absolute_error: 0.0032\n",
      "Epoch 88/100\n",
      "127500/127500 [==============================] - 38s 301us/step - loss: 2.6976e-05 - mean_absolute_error: 0.0032\n",
      "Epoch 89/100\n",
      "127500/127500 [==============================] - 38s 302us/step - loss: 2.5755e-05 - mean_absolute_error: 0.0033\n",
      "Epoch 90/100\n",
      "127500/127500 [==============================] - 38s 301us/step - loss: 2.5079e-05 - mean_absolute_error: 0.0034\n",
      "Epoch 91/100\n",
      "127500/127500 [==============================] - 39s 310us/step - loss: 2.9864e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 92/100\n",
      "127500/127500 [==============================] - 39s 310us/step - loss: 2.5588e-05 - mean_absolute_error: 0.0032\n",
      "Epoch 93/100\n",
      "127500/127500 [==============================] - 37s 287us/step - loss: 2.5455e-05 - mean_absolute_error: 0.0030\n",
      "Epoch 94/100\n",
      "127500/127500 [==============================] - 38s 296us/step - loss: 2.7191e-05 - mean_absolute_error: 0.0034\n",
      "Epoch 95/100\n",
      "127500/127500 [==============================] - 39s 307us/step - loss: 2.8701e-05 - mean_absolute_error: 0.0034\n",
      "Epoch 96/100\n",
      "127500/127500 [==============================] - 39s 303us/step - loss: 2.4530e-05 - mean_absolute_error: 0.0032\n",
      "Epoch 97/100\n",
      "127500/127500 [==============================] - 39s 306us/step - loss: 2.3720e-05 - mean_absolute_error: 0.0032\n",
      "Epoch 98/100\n",
      "127500/127500 [==============================] - 39s 309us/step - loss: 2.5156e-05 - mean_absolute_error: 0.0033\n",
      "Epoch 99/100\n",
      "127500/127500 [==============================] - 40s 315us/step - loss: 2.5931e-05 - mean_absolute_error: 0.0035\n",
      "Epoch 100/100\n",
      "127500/127500 [==============================] - 43s 340us/step - loss: 2.5826e-05 - mean_absolute_error: 0.0032\n",
      "22500/22500 [==============================] - 11s 507us/step\n",
      "Loss Funcion Minima On test dataset : 0.00000208\n",
      "Mean Absolute Error On test dataset : 0.00111805\n"
     ]
    }
   ],
   "source": [
    "# Running The Model\n",
    "\n",
    "# Setting the optimizer parameters\n",
    "opt = optimizers.Nadam(lr=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "# compiling the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt,\n",
    "              metrics=['mae'])\n",
    "# fitting the data with model (learning process)\n",
    "model.fit(data, labels, epochs=epochs, batch_size=batch_size)  \n",
    "# evaluating the results\n",
    "loss, MAE= model.evaluate(d_test, l_test)\n",
    "# printing the final results on test data set\n",
    "print('Loss Funcion Minima On test dataset : %.8f' % (loss))\n",
    "print('Mean Absolute Error On test dataset : %.8f' % (MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to make some predictions, and we do some error analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of predicted A_eff : 0.76631910\n",
      "norm of the mean of the commited error : 0.00066640\n"
     ]
    }
   ],
   "source": [
    "predicted_A_eff = model.predict(d_test,batch_size=150)\n",
    "print('mean of predicted A_eff : %.8f' % (np.mean(predicted_A_eff)))\n",
    "print('norm of the mean of the commited error : %.8f' % (np.linalg.norm(np.mean(predicted_A_eff) - np.mean(l_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the norm of a vector created whose elements are the commited error over all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of L2 commited error: 0.21650887\n"
     ]
    }
   ],
   "source": [
    "err =[]\n",
    "for i in range(len(predicted_A_eff)):\n",
    "    err.append(predicted_A_eff[i]-l_test[i])\n",
    "err = np.array(err)\n",
    "print('mean of L2 commited error: %.8f'%(np.linalg.norm(err)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to see the out put of the first part. and verify if it looks like a reciprocal function or not. References to look at for this work:\n",
    "\n",
    "* https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer\n",
    "\n",
    "* https://itnspotlight.com/dissecting-keras-neural-networks-accessing-weights-and-hidden-layers/\n",
    "\n",
    "* https://keras.io/layers/about-keras-layers/\n",
    "\n",
    "* https://keras.io/models/about-keras-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 8, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 8, 16)        32          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 16)        0           conv1d_9[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 8, 16)        272         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 16)        0           conv1d_10[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 8, 16)        272         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 16)        0           conv1d_11[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 8, 1)         17          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 1)         0           conv1d_12[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 1)            0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 1)         0           lambda_2[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 8, 16)        32          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 16)        0           conv1d_13[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 8, 16)        272         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 16)        0           conv1d_14[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 8, 16)        272         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 16)        0           conv1d_15[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 8, 1)         17          add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            9           flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,195\n",
      "Trainable params: 1,195\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get this layer name, use a model.summary() and pick the ID\n",
    "layer_name = 'conv1d_12'\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets process data a bit before ploting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = d_test.reshape((d_test.shape[0],d_test.shape[2],d_test.shape[1])).flatten()\n",
    "ys = intermediate_output.reshape((intermediate_output.shape[0],intermediate_output.shape[2],intermediate_output.shape[1])).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not sort our dictionary, the plot would look like a childs drawing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.30000123, -18.94641113],\n",
       "       [  0.30000996, -18.94593048],\n",
       "       [  0.30001437, -18.94569206],\n",
       "       ...,\n",
       "       [  1.49996473,   4.3779583 ],\n",
       "       [  1.49998212,   4.37802744],\n",
       "       [  1.49999522,   4.37808084]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1 = np.hstack((xs.reshape((xs.shape[0],1)), ys.reshape((ys.shape[0],1))))\n",
    "dict1.sort(axis=0)\n",
    "np.unique(dict1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the plot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samim/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: RankWarning: Polyfit may be poorly conditioned\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ff7fe6d50>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(dict1[:,0], dict1[:,1], '-', color='grey', alpha=.3,  label='predicted')\n",
    "x = dict1[:,0]\n",
    "y = dict1[:,1]\n",
    "z = np.polyfit(x, y, 15)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "tpp =[]\n",
    "for i in dict1[:,0]:\n",
    "    tpp.append(p(i))  \n",
    "\n",
    "plt.plot(x, tpp,'-', alpha=1,  label='fitted')\n",
    "\n",
    "# here is more nice options for plotting, But I prefered to keep it simple\n",
    "\n",
    "#plt.grid(axis='x', color='0.95')\n",
    "#plt.legend(title='method')\n",
    "#plt.title('-1 * output of the second stage')\n",
    "\n",
    "#font = {'family': 'serif',\n",
    "#        'color':  'gray',\n",
    "#        'weight': 'normal',\n",
    "#        'size': 16,\n",
    "#        }\n",
    "\n",
    "\n",
    "#plt.xlabel('input of the network', fontdict=font)\n",
    "#plt.ylabel('first stage output', fontdict=font)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save this oplots data for later uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/fitted.csv', [a[0], tpp],delimiter=',')\n",
    "np.savetxt('data/predicted.csv', [a[0], toplot],delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the later plot processing, I find this to be interesting: accessing the weights and biases of a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wights: ', array([[[ 0.3299515 ],\n",
      "        [ 0.61033154],\n",
      "        [-0.91858023],\n",
      "        [ 0.79258454],\n",
      "        [ 0.33547732],\n",
      "        [ 0.367044  ],\n",
      "        [ 0.51160383],\n",
      "        [ 0.2537765 ],\n",
      "        [-0.22552976],\n",
      "        [ 0.44135788],\n",
      "        [ 0.4447429 ],\n",
      "        [ 0.7309333 ],\n",
      "        [ 0.59254324],\n",
      "        [ 0.11585502],\n",
      "        [-0.42361298],\n",
      "        [-0.32061455]]], dtype=float32))\n",
      "('bias: ', array([-1.5213469], dtype=float32))\n",
      "('config: ', {'padding': 'same', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'scale': 1.0, 'seed': None, 'mode': 'fan_avg'}}, 'name': 'conv1d_4', 'bias_regularizer': None, 'filters': 1, 'bias_constraint': None, 'activation': 'linear', 'trainable': True, 'data_format': 'channels_last', 'kernel_constraint': None, 'strides': (1,), 'dilation_rate': (1,), 'kernel_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'use_bias': True, 'activity_regularizer': None, 'kernel_size': (1,)})\n"
     ]
    }
   ],
   "source": [
    "print(\"wights: \" ,model.get_layer(\"conv1d_4\").get_weights()[0])\n",
    "print(\"bias: \",model.get_layer(\"conv1d_4\").get_weights()[1])\n",
    "print(\"config: \",model.get_layer(\"conv1d_4\").get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets save the model to the disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
